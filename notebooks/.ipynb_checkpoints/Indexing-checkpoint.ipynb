{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTDiIByL_vI7"
   },
   "source": [
    "# Indexing and Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z73OQElJ_vI-"
   },
   "source": [
    "**Indexing and search** : This step prepares an indexed database which facilitates the entity search and counting operation . First, initialize the ElasticSearch index object. Then the index is populated in batches with bulk indexing functionality available in Elasticsearch package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Im848bLPWdo8"
   },
   "source": [
    "**Installation of required python package** : Install and import  elasticsearch and elasticsearch_dsl library to the current python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "vO9ylFC5WjNT"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Search, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JdE-ggzWWjdU"
   },
   "source": [
    "**Index initialization** : Index initialization is done with the index information which includes index name, type name, number of shards, number of replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BXF6l4hv_vI_"
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"pubmed\"\n",
    "TYPE_NAME = \"pubmed_meta\"\n",
    "NUMBER_SHARDS = 1 # keep this as one if no clusterNUMBER_REPLICAS = 0 \n",
    "'''\n",
    "    following is the defined schema\n",
    "    totally 5 fields: \n",
    "    pmid, date, author list, journal name, mesh heading list\n",
    "'''\n",
    "request_body = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": NUMBER_SHARDS,\n",
    "            \"number_of_replicas\": NUMBER_REPLICAS\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            TYPE_NAME: {\n",
    "                \"properties\": {\n",
    "                    \"pmid\": {\n",
    "                        \"type\": \"keyword\"\n",
    "                    },\n",
    "                    # \"date\": {\n",
    "                    #     \"type\": \"long\"\n",
    "                    # },\n",
    "                    # \"author_list\": {\n",
    "                    #     \"type\": \"keyword\"\n",
    "                    # },\n",
    "                    # \"journal_name\": {\n",
    "                    #     \"type\": \"keyword\"\n",
    "                    # },\n",
    "                    \"mesh_heading\": {\n",
    "                        \"type\": \"text\",\n",
    "                        \"similarity\": \"BM25\"\n",
    "                    },\n",
    "                    \"abstract\":{\n",
    "                        \"type\": \"text\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "es = Elasticsearch()\n",
    "if es.indices.exists(INDEX_NAME):\n",
    "     res = es.indices.delete(index = INDEX_NAME)\n",
    "     print(\"Deleting index %s , Response: %s\" % (INDEX_NAME, res))\n",
    "res = es.indices.create(index = INDEX_NAME, body = request_body)\n",
    "print(\"Create index %s , Response: %s\" % (INDEX_NAME, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0c36KmlT_vJF"
   },
   "source": [
    "**Bulk indexing** : \n",
    "In the first step of bulk indexing, data bulk is created with two components. \n",
    "- First component is a dictionary with metadata information of index name, type name and bluck id  which is  ‘pmid’  key.  \n",
    "- Prepare the  second component which  is  data dictionary with all information of ‘title’,’abstract’,’MeSH’ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Gsy3QOpQ_vJH"
   },
   "outputs": [],
   "source": [
    "inputFilePath = \"./pubmed.json\"\n",
    "logFilePath = \"./index_pubmed_log_20171001.txt\"\n",
    "\n",
    "    \n",
    "INDEX_NAME = \"pubmed\"\n",
    "TYPE_NAME = \"pubmed_meta\"\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "mesh2pmid = dict()\n",
    "ic = 0\n",
    "ir = 0\n",
    "\n",
    " with open(inputFilePath, \"r\") as fin, open(logFilePath, \"w\") as fout:\n",
    "        start = time.time()\n",
    "        bulk_size = 5000 # number of document processed in each bulk index\n",
    "        bulk_data = [] # data in bulk index\n",
    "\n",
    "        cnt = 0\n",
    "        for line in fin: ## each line is single document\n",
    "            try:\n",
    "                cnt += 1\n",
    "                paperInfo = json.loads(line.strip())\n",
    "                \n",
    "                data_dict = {}\n",
    "                \n",
    "                # update PMID\n",
    "                data_dict[\"pmid\"] = paperInfo.get(\"PMID\", \"-1\")\n",
    "                \n",
    "        \n",
    "                \n",
    "\n",
    "                #update MeSH Heading <----------------------\n",
    "                data_dict[\"mesh_heading\"] = \" \".join(paperInfo[\"MeshHeadingList\"])\n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                # update Abstract<------------------\n",
    "                data_dict[\"abstract\"] = paperInfo.get(\"Abstract\", \"\").lower().replace('-', ' ')\n",
    "\n",
    "                        \n",
    "                \n",
    "                ## Put current data into the bulk <---------\n",
    "                op_dict = {\n",
    "                    \"index\": {\n",
    "                        \"_index\": INDEX_NAME,\n",
    "                        \"_type\": TYPE_NAME,\n",
    "                        \"_id\": data_dict[\"pmid\"]\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                bulk_data.append(op_dict)\n",
    "                bulk_data.append(data_dict) \n",
    "                \n",
    "                \n",
    "                \n",
    "                        \n",
    "                ## Start Bulk indexing\n",
    "                if cnt % bulk_size == 0 and cnt != 0:\n",
    "                    ic += 1\n",
    "                    tmp = time.time()\n",
    "                    es.bulk(index=INDEX_NAME, body=bulk_data, request_timeout = 500)\n",
    "                    fout.write(\"bulk indexing... %s, escaped time %s (seconds) \\n\" \\\n",
    "                               % ( cnt, tmp - start ) )\n",
    "                    \n",
    "                    if ic%100 ==0:\n",
    "                        print(\" i bulk indexing... %s, escaped time %s (seconds) \" \\\n",
    "                              % ( cnt, tmp - start ) )\n",
    "                    \n",
    "                    \n",
    "                    bulk_data = []\n",
    "                    \n",
    "                    \n",
    "\n",
    "            except:\n",
    "                cnt -= 1\n",
    "                print(\"XXXX Unexpected Error happened at line: XXXX\")\n",
    "                #print(line)\n",
    "                \n",
    "                \n",
    "        \n",
    "        ## indexing those left papers\n",
    "        if bulk_data:\n",
    "            ir +=1\n",
    "            tmp = time.time()\n",
    "            es.bulk(index=INDEX_NAME, body=bulk_data, request_timeout = 500)\n",
    "            fout.write(\"bulk indexing... %s, escaped time %s (seconds) \\n\"\\\n",
    "                       % ( cnt, tmp - start ) )\n",
    "            \n",
    "            if ir%100 ==0:\n",
    "                print(\" r bulk indexing... %s, escaped time %s (seconds) \"\\\n",
    "                      % ( cnt, tmp - start ) )\n",
    "            bulk_data = []\n",
    "            \n",
    "        \n",
    "\n",
    "        end = time.time()\n",
    "        fout.write(\"Finish PubMed meta-data indexing. Total escaped time %s (seconds) \\n\"\\\n",
    "                   % (end - start) )\n",
    "        print(\"Finish PubMed meta-data indexing. Total escaped time %s (seconds) \"\\\n",
    "              % (end - start) )\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_dG7UMuGXkds"
   },
   "source": [
    "**Search functionality** : One can perform search operation over data index created by Elasticsearch application. Once a search operation is initiated for user defined entity, it gathers information of that entity as a ranked list. Following are the steps for search operation:\n",
    "- start the Elasticsearch server\n",
    "- implement Elasticsearch DSL search functionality with index name, parameters and query\n",
    "- iterate over all hits obtained in search result to find desired entity"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "3. Indexing.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
