{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CNbXs2L82S_"
   },
   "source": [
    "# Data Parsingn Pipeline\n",
    "\n",
    "This pipeline will parse the extracted data and convert it into data structures compatible with the CaseOLAP pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKAemD7ITsnq"
   },
   "source": [
    "**Installation of required python package** : Install and import ```lxml, itertools``` and ```json```  libraries into the current python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "gwWLx4N2Tx2Y"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Ea3BsJS82TD"
   },
   "source": [
    "###  Set up output data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "z_UJ4ofhUB5G"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_629A6OUFuj"
   },
   "source": [
    "MeSH statistics dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "dn7gJMNrUM8y"
   },
   "outputs": [],
   "source": [
    "mesh_statistics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5t36lQ4WUdGu"
   },
   "source": [
    "**Data parsing strategy ** :  The extracted data is an ```XML``` file, and text data is embedded in the tree structure of ```XML``` document. The following are steps for parsing data:\n",
    "- Implement  the etree functionality  in ```lxml``` module to dig into the tree structure of ```XML``` document. \n",
    "- The separate components e.g. ```PMID, authors, abstract```, ```MeSH``` etc. of the data is obtained by using tags representing these components. \n",
    "- Implement the chain functionality in itertools  to creates an iterator that returns elements from the iterables which was  obtained by implementing  etree functionality in ```lxml```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JBkz9N-G82TE"
   },
   "outputs": [],
   "source": [
    "# Search the tag in the xml element\n",
    "# Return tag's text if tag exists, return empty string if doesn't\n",
    "def get_text(element, tag):\n",
    "    e = element.find(tag)\n",
    "    if e is not None:\n",
    "        return e.text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# <!ELEMENT\tAuthorList (Author+) >\n",
    "# <!ELEMENT\tAuthor (((LastName, ForeName?, Initials?, Suffix?)\\\n",
    "#                | CollectiveName), Identifier*, AffiliationInfo*) >\n",
    "def parse_author(authors):\n",
    "    result = []\n",
    "    for author in authors:\n",
    "        item = {}\n",
    "        item['LastName'] = get_text(author, 'LastName')\n",
    "        item['ForeName'] = get_text(author, 'ForeName')\n",
    "        item['Initials'] = get_text(author, 'Initials')\n",
    "        item['Suffix'] = get_text(author, 'Suffix')\n",
    "        item['CollectiveName'] = get_text(author, 'CollectiveName')\n",
    "        result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "auc_UktxVa0y"
   },
   "source": [
    "**Creation of dictionary of parsed data **: A python dictionary  is created with all the components as key-value pair. This JSON-like data structure makes it compatible for  indexing and searching  in Elasticsearch which is described in step 3 of protocol.\n",
    "\n",
    "**Creation of MeSH to PMID mapping**: During the creation of dictionary of parsed data, MeSH to PMID mapping table can also be created. This mapping is used to create Text-cube(in step 4) document structure as an requirement of CaseOLAP algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "XWK_Dcja82TL"
   },
   "outputs": [],
   "source": [
    "def parse_pubmed_file(file, pubmed_output_file, pmid2mesh_output_file):\n",
    "\n",
    "    print('Start parsing %s' % file)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    f = open(file, 'r')\n",
    "\n",
    "    tree = etree.parse(f)\n",
    "    articles = itertools.chain(tree.findall('PubmedArticle'), tree.findall('BookDocument'))\n",
    "    count = 0\n",
    "\n",
    "    noabs = 0\n",
    "    \n",
    "    for article in articles:\n",
    "\n",
    "        count += 1\n",
    "        result = {}\n",
    "        pmid2mesh = {}\n",
    "\n",
    "        # PMID - Exactly One Occurrance\n",
    "        result['PMID'] = get_text(article, './/PMID')\n",
    "        pmid2mesh['PMID'] = get_text(article, './/PMID')\n",
    "\n",
    "        # # Article title - Zero or One Occurrences\n",
    "        # result['ArticleTitle'] = get_text(article, './/ArticleTitle')\n",
    "\n",
    "        # Abstract - Zero or One Occurrences\n",
    "        abstractList = article.find('.//Abstract')\n",
    "        if abstractList != None:\n",
    "            try:\n",
    "                abstract = '\\n'.join([line.text for line in abstractList.\\\n",
    "                                      findall('AbstractText')])\n",
    "                result['Abstract'] = abstract\n",
    "            except:\n",
    "                result['Abstract'] = ''\n",
    "                noabs += 1\n",
    "        else:\n",
    "            result['Abstract'] = ''\n",
    "            noabs += 1\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "        # # Author List - Zero or More Occurrences\n",
    "        # authors = article.findall('.//Author')\n",
    "        # result['AuthorList'] = parse_author(authors)\n",
    "        \n",
    "        # # Journal - Exactly One Occurrance\n",
    "        # journal = article.find('.//Journal')\n",
    "        # result['Journal'] = get_text(journal, 'Title')\n",
    "        \n",
    "        \n",
    "        result['PubDate'] = {}\n",
    "        result['PubDate']['Year'] = get_text(journal, 'JournalIssue/PubDate/Year')\n",
    "        \n",
    "        # result['PubDate']['Month'] = get_text(journal, 'JournalIssue/PubDate/Month')\n",
    "        # result['PubDate']['Day'] = get_text(journal, 'JournalIssue/PubDate/Day')\n",
    "        # result['PubDate']['Season'] = get_text(journal, 'JournalIssue/PubDate/Season')\n",
    "        # result['PubDate']['MedlineDate'] = get_text(journal,\\\n",
    "        #                                   'JournalIssue/PubDate/MedlineDate')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # MeshHeading - Zero or More Occurrences\n",
    "        headings = article.findall('.//MeshHeading')\n",
    "        \n",
    "        result['MeshHeadingList'] = []\n",
    "        pmid2mesh['MeshHeadingList'] = []\n",
    "        if headings:\n",
    "            for heading in headings:\n",
    "                descriptor_names = heading.findall('DescriptorName')\n",
    "                qualifier_names = heading.findall('QualifierName')\n",
    "                if descriptor_names:\n",
    "                    for descriptor_name in descriptor_names:\n",
    "                        result['MeshHeadingList'].append(descriptor_name.text)\n",
    "                        pmid2mesh['MeshHeadingList'].append(descriptor_name.text)\n",
    "                if qualifier_names:\n",
    "                    for qualifier_name in qualifier_names:\n",
    "                        result['MeshHeadingList'].append(qualifier_name.text)\n",
    "                        pmid2mesh['MeshHeadingList'].append(qualifier_name.text)\n",
    "                        \n",
    "                        \n",
    "        \n",
    "        \n",
    "        mesh_count = len(result['MeshHeadingList'])\n",
    "        \n",
    "        if mesh_count in mesh_statistics:\n",
    "            mesh_statistics[mesh_count] += 1\n",
    "        else:\n",
    "            mesh_statistics[mesh_count] = 1\n",
    "\n",
    "        # Dump to pubmed json file <----------------------------\n",
    "        json.dump(result, pubmed_output_file)\n",
    "        pubmed_output_file.write('\\n')\n",
    "        \n",
    "        # Dump to pmid2mesh json file <-------------------------\n",
    "        json.dump(pmid2mesh, pmid2mesh_output_file)\n",
    "        pmid2mesh_output_file.write('\\n')\n",
    "\n",
    "\n",
    "    print('Finish parsing %s, totally %d articles parsed. Total time: %fs'\\\n",
    "                             % (file, count, time.time() - t1))\n",
    "    print('%d acticles no abstracts' % (noabs))\n",
    "    sys.stdout.flush()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TIExZC33U8ni"
   },
   "source": [
    "**Setting up directories in parsing loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "YFe9rrJa82TN"
   },
   "outputs": [],
   "source": [
    "def parse_dir(source_dir, pubmed_output_file,pmid2mesh_output_file):\n",
    "    if os.path.isdir(source_dir):\n",
    "        for file in os.listdir(source_dir):\n",
    "            if re.search(r'^pubmed18n\\d\\d\\d\\d.xml$', file) is not None:\n",
    "                try:\n",
    "                    parse_pubmed_file(os.path.join(source_dir, file),\\\n",
    "                                      pubmed_output_file, pmid2mesh_output_file)\n",
    "                except:\n",
    "                    print(\"XXXX Unexpected error happended when parsing %s XXXX\" % file)\n",
    "                    print(traceback.print_exc())\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JODWE1dzU_TH"
   },
   "source": [
    "**Run the Parsing Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "misI_P2R82TS"
   },
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "pubmed_output_file_path = os.path.join(DATA_DIR, 'data/pubmed.json')\n",
    "pmid2mesh_output_file_path = os.path.join(DATA_DIR, 'pmid2mesh/pmid2mesh_from_parsing.json')\n",
    "    \n",
    "    \n",
    "    \n",
    "pubmed_output_file = open(pubmed_output_file_path, 'w')\n",
    "pmid2mesh_output_file = open(pmid2mesh_output_file_path, 'w')\n",
    "    \n",
    "    \n",
    "    \n",
    "parse_dir(os.path.join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/baseline'),\\\n",
    "              pubmed_output_file, pmid2mesh_output_file)\n",
    "parse_dir(os.path.join(DATA_DIR, 'ftp.ncbi.nlm.nih.gov/pubmed/updatefiles'),\\\n",
    "              pubmed_output_file, pmid2mesh_output_file)\n",
    "    \n",
    "    \n",
    "    \n",
    "pubmed_output_file.close()\n",
    "pmid2mesh_output_file.close()\n",
    "\n",
    "mesh_file = open(os.path.join(DATA_DIR, 'data/mesh_statistics.json'), 'w')\n",
    "json.dump(mesh_statistics, mesh_file)\n",
    "mesh_file.close()\n",
    "\n",
    "print(\"==== Parsing finished, results dumped to %s ====\" % pubmed_output_file_path)\n",
    "print(\"==== TOTAL TIME: %fs ====\" % (time.time() - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeSH to PMID Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFilePath = \"data/pubmed.json\"\n",
    "meshFilePath = \"mesh2pmid/\"\n",
    "    \n",
    "mesh2pmid_output_file = open(meshFilePath + 'mesh2pmid.json', \"w\") \n",
    "mesh2pmid = dict()\n",
    "    \n",
    "    with open(inputFilePath, \"r\") as fin:\n",
    "        \n",
    "        start = time.time()\n",
    "        k = 0\n",
    "            \n",
    "        for line in fin: ## each line is single document\n",
    "            \n",
    "            try:\n",
    "                k = k+1\n",
    "                paperInfo = json.loads(line.strip())\n",
    "                \n",
    "                data_dict = {}\n",
    "                \n",
    "                # update PMID\n",
    "                data_dict[\"pmid\"] = paperInfo.get(\"PMID\", \"-1\")\n",
    "                \n",
    "                #update MeSH Heading <----------------------\n",
    "                data_dict[\"mesh_heading\"] = \" \".join(paperInfo[\"MeshHeadingList\"])\n",
    "                \n",
    "                # collect Mesh2PMID <-------------------\n",
    "                if data_dict[\"pmid\"] != \"-1\":\n",
    "                    for mesh in paperInfo[\"MeshHeadingList\"]:\n",
    "                        if mesh not in mesh2pmid:\n",
    "                               mesh2pmid[mesh] = []\n",
    "                        mesh2pmid[mesh].append(data_dict[\"pmid\"])\n",
    "                        \n",
    "                        \n",
    "                          \n",
    "                if k%500000 == 0:\n",
    "                    print(k,'done!')\n",
    "                    #break\n",
    "                    \n",
    "            except:\n",
    "                print(\"XXXX Unexpected Error happened at line: XXXX\")\n",
    "               \n",
    "                \n",
    "          \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Dumping rest papers\n",
    "        for key,value in mesh2pmid.items():\n",
    "            json.dump({key:value}, mesh2pmid_output_file)\n",
    "            mesh2pmid_output_file.write('\\n')\n",
    "            \n",
    "        mesh2pmid = dict()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        end = time.time()\n",
    "        print(\"Finish Total escaped time %s (seconds) \" % (end - start) )\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Parsing.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
